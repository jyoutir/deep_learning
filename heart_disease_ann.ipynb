{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction using Artificial Neural Network\n",
    "\n",
    "This notebook implements a neural network from scratch for heart disease prediction, following these tasks:\n",
    "1. Model Implementation\n",
    "2. Model Evaluation\n",
    "3. Model Performance Optimization\n",
    "\n",
    "## Dataset Description\n",
    "The Heart Disease Dataset contains 303 samples with 13 features:\n",
    "- age\n",
    "- sex\n",
    "- chest pain type\n",
    "- resting blood pressure\n",
    "- cholesterol levels\n",
    "- fasting blood sugar\n",
    "- resting electrocardiographic results\n",
    "- maximum heart rate achieved\n",
    "- exercise induced angina\n",
    "- ST depression induced by exercise relative to rest\n",
    "- slope of the peak exercise ST segment\n",
    "- number of major vessels colored by fluoroscopy\n",
    "- athalassemia type\n",
    "\n",
    "Target: Binary classification (0 – no risk, 1 – at risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Model Implementation\n",
    "\n",
    "### Neural Network Architecture\n",
    "- Input Layer: 13 neurons (one for each feature)\n",
    "- Hidden Layer: 8 neurons with sigmoid activation\n",
    "- Output Layer: 1 neuron with sigmoid activation (binary classification)\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Forward Propagation\n",
    "1. Hidden Layer:\n",
    "   - Z₁ = X·W₁ + b₁ (linear transformation)\n",
    "   - A₁ = sigmoid(Z₁) (activation)\n",
    "\n",
    "2. Output Layer:\n",
    "   - Z₂ = A₁·W₂ + b₂\n",
    "   - A₂ = sigmoid(Z₂)\n",
    "\n",
    "#### Backward Propagation\n",
    "1. Output Layer Gradients:\n",
    "   - dZ₂ = A₂ - y\n",
    "   - dW₂ = (1/m) * A₁ᵀ·dZ₂\n",
    "   - db₂ = (1/m) * Σ(dZ₂)\n",
    "\n",
    "2. Hidden Layer Gradients:\n",
    "   - dZ₁ = W₂·dZ₂ᵀ ⊙ sigmoid_derivative(A₁)\n",
    "   - dW₁ = (1/m) * Xᵀ·dZ₁\n",
    "   - db₁ = (1/m) * Σ(dZ₁)\n",
    "\n",
    "#### Loss Function\n",
    "Binary Cross-Entropy:\n",
    "L = -(1/m) * Σ(y·log(ŷ) + (1-y)·log(1-ŷ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Neural Network Implementation\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights using uniform distribution between -1 and 1\n",
    "        self.W1 = np.random.uniform(-1, 1, (input_size, hidden_size))\n",
    "        self.W2 = np.random.uniform(-1, 1, (hidden_size, output_size))\n",
    "        \n",
    "        # Initialize biases to 0\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def sigmoid_derivative(self, A):\n",
    "        return A * (1 - A)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.sigmoid(self.Z1)\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        return self.A2\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        dZ2 = self.A2 - y\n",
    "        dW2 = (1/m) * np.dot(self.A1.T, dZ2)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * self.sigmoid_derivative(self.A1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "        \n",
    "        return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        self.W1 -= self.learning_rate * gradients[\"dW1\"]\n",
    "        self.b1 -= self.learning_rate * gradients[\"db1\"]\n",
    "        self.W2 -= self.learning_rate * gradients[\"dW2\"]\n",
    "        self.b2 -= self.learning_rate * gradients[\"db2\"]\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        loss = -(1/m) * np.sum(y_true * np.log(y_pred + 1e-15) + \n",
    "                              (1 - y_true) * np.log(1 - y_pred + 1e-15))\n",
    "        return loss\n",
    "    \n",
    "    def get_accuracy(self, X, y):\n",
    "        predictions = (self.forward_propagation(X) >= 0.5).astype(int)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def train(self, X, y, epochs, batch_size):\n",
    "        for epoch in range(epochs):\n",
    "            # Create mini-batches\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                # Forward propagation\n",
    "                y_pred = self.forward_propagation(X_batch)\n",
    "                \n",
    "                # Backward propagation\n",
    "                gradients = self.backward_propagation(X_batch, y_batch)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(gradients)\n",
    "            \n",
    "            # Compute epoch metrics\n",
    "            y_pred_all = self.forward_propagation(X)\n",
    "            epoch_loss = self.compute_loss(y, y_pred_all)\n",
    "            epoch_accuracy = self.get_accuracy(X, y)\n",
    "            \n",
    "            self.loss_history.append(epoch_loss)\n",
    "            self.accuracy_history.append(epoch_accuracy)\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "                print(f\"Loss: {epoch_loss:.4f}\")\n",
    "                print(f\"Accuracy: {epoch_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and preprocess data\n",
    "data = pd.read_csv('heart-disease.csv')\n",
    "X = data.drop('target', axis=1).values\n",
    "y = data['target'].values.reshape(-1, 1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train model with specified parameters\n",
    "model = NeuralNetwork(\n",
    "    input_size=13,\n",
    "    hidden_size=8,\n",
    "    output_size=1,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Train with specified parameters\n",
    "model.train(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    epochs=500,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Evaluation\n",
    "\n",
    "### Evaluation Strategy\n",
    "1. Split data into training (80%) and test (20%) sets\n",
    "2. Use multiple evaluation metrics:\n",
    "   - Binary Cross-Entropy Loss\n",
    "   - Classification Accuracy\n",
    "   - Precision, Recall, F1-Score\n",
    "   - ROC Curve and AUC\n",
    "   - Confusion Matrix\n",
    "3. Analyze training stability through multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_training_metrics(model):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(model.loss_history)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary Cross-Entropy Loss')\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(model.accuracy_history)\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot training metrics\n",
    "plot_training_metrics(model)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_proba = model.forward_propagation(X_test)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model Performance Optimization\n",
    "\n",
    "### Optimization Strategies\n",
    "1. Experiment with different activation functions:\n",
    "   - Sigmoid (baseline)\n",
    "   - ReLU\n",
    "   - Tanh\n",
    "\n",
    "2. Vary hidden layer neurons:\n",
    "   - 4 neurons\n",
    "   - 8 neurons (baseline)\n",
    "   - 16 neurons\n",
    "   - 32 neurons\n",
    "\n",
    "3. Test different learning rates:\n",
    "   - 0.001\n",
    "   - 0.01 (baseline)\n",
    "   - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from task3_optimization import OptimizedNeuralNetwork\n",
    "\n",
    "def experiment_activation_functions():\n",
    "    activations = ['sigmoid', 'relu', 'tanh']\n",
    "    results = []\n",
    "    \n",
    "    for activation in activations:\n",
    "        model = OptimizedNeuralNetwork(\n",
    "            input_size=13,\n",
    "            hidden_size=8,\n",
    "            output_size=1,\n",
    "            learning_rate=0.01,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "        model.train(X_train, y_train, epochs=500, batch_size=128)\n",
    "        test_acc = model.get_accuracy(X_test, y_test)\n",
    "        \n",
    "        results.append({\n",
    "            'activation': activation,\n",
    "            'test_acc': test_acc,\n",
    "            'loss_history': model.loss_history\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nActivation: {activation}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "activation_results = experiment_activation_functions()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "for result in activation_results:\n",
    "    plt.plot(result['loss_history'], label=result['activation'])\n",
    "plt.title('Training Loss by Activation Function')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Test accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "activations = [r['activation'] for r in activation_results]\n",
    "accuracies = [r['test_acc'] for r in activation_results]\n",
    "plt.bar(activations, accuracies)\n",
    "plt.title('Test Accuracy by Activation Function')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}